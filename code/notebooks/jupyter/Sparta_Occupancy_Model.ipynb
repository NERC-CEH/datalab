{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sparta - Occupancy Model\n",
    "This example will distruibute multiple Sparta model fits across a cluster using Spark/SparkR technology.\n",
    "\n",
    "By using the \"`R (SparkR)`\" jupyter kernel the SparkR libary is loaded and the Spark context (sc) generated automatically. These will need to be set by the user if not using the SparkR jupyter kernel (see commands below). Where the container environmental varaibles `SPARK_HOME` and `MASTER`  give the path to local Spark installation and address of the Spark cluster, respectively.\n",
    " \n",
    " ```\n",
    " library(SparkR, lib.loc=file.path(Sys.getenv('SPARK_HOME'), 'R', 'lib'))\n",
    " sc <- sparkR.session(master=Sys.getenv('MASTER'))\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'sc'"
      ],
      "text/latex": [
       "'sc'"
      ],
      "text/markdown": [
       "'sc'"
      ],
      "text/plain": [
       "[1] \"sc\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ls() # Spark context (sc) already generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `devtools` library is install by default to allow for installing new libraries. At present if the container resets the new libray will be lost. Libraries can be installed globally on request, which will persist following a restart of the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping install of 'sparta' from a github remote, the SHA1 (f7ea39de) has not changed since last install.\n",
      "  Use `force = TRUE` to force installation\n"
     ]
    }
   ],
   "source": [
    "library(devtools)\n",
    "install_github('BiologicalRecordsCentre/sparta@0.1.30')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User files **must be** written and read from the `/data/` path as this is availble to availible to all the containers (notebooks and Spark nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: lme4\n",
      "Loading required package: Matrix\n"
     ]
    }
   ],
   "source": [
    "library(sparta)\n",
    "load('/data/biodiversity/taxa/input/Test_Data.rdata')\n",
    "visitData <- formatOccData(taxa = taxa_data$CONCEPT, site = taxa_data$TO_GRIDREF, time_period = taxa_data$TO_STARTDATE)\n",
    "save(visitData, file = '/data/biodiversity/taxa/input/visit_data.rdata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries used within functions to be distributed to a Spark cluster **must be** install on the Spark worker and then import them inside the function. At present, libraries can only be installed by request. For this example, the `Sparta` libary (frozen at version 0.1.30) `R2jags` have been installed on the Spark workers.\n",
    "\n",
    "Only a single argument can be passed to the distributed functions using `spark.lapply` command. More complex structures can passed using lists, but are limited in Jupyter IO flow. For this example, save `visitData` from current notebook and re-load it within the worker function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelFitFunct <-  function(taxaName) {\n",
    "    start.time <- Sys.time()\n",
    "    loadNamespace(\"sparta\") # this will throw an error if the library is missing\n",
    "    loadNamespace(\"R2jags\")\n",
    "    load('/data/biodiversity/taxa/input/visit_data.rdata') # Load required data *contains variable visitData*\n",
    "    iterations <- 10\n",
    "    model.output <- sparta::occDetFunc(taxa_name = taxaName,\n",
    "                                       occDetdata = visitData$occDetdata,\n",
    "                                       spp_vis = visitData$spp_vis,\n",
    "                                       write_results = FALSE,\n",
    "                                       n_chains = 1,\n",
    "                                       n_iterations = iterations,\n",
    "                                       burnin = iterations/2,\n",
    "                                       thinning = 3,\n",
    "                                       nyr = 2)\n",
    "    # Save output to the /data/ path on each loop, in case the browser tab is closed\n",
    "    save(model.output, file = file.path('/data/biodiversity/taxa/output', paste0(taxaName, '.rdata')))\n",
    "    end.time <- Sys.time()\n",
    "    return(end.time - start.time)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike Zeppelin, Jupyter Notebooks can not keep a process running once the browser tab is closed. The `Future` library, will turn a process into an asynchronous task and will allow the cell to continue to run in the backgound. Variables can then be checked using the `value` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(\"future\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runList <- tail(colnames(visitData$spp_vis), -1)[1:2] # limit to 2 species rather than the full 50\n",
    "out <- future({spark.lapply(runList, modelFitFunct)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R (SparkR)",
   "language": "R",
   "name": "r-spark"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
