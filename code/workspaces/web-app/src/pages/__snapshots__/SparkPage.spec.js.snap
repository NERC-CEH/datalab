// Jest Snapshot v1, https://goo.gl/fbAQLP

exports[`SparkPage renders correct snapshot 1`] = `
<WithStyles(Page)
  className=""
  title="Spark"
>
  <WithStyles(ForwardRef(Typography))
    variant="body1"
  >
    <ExternalLink
      href="https://spark.apache.org/"
    >
      Apache Spark
    </ExternalLink>
     is an open-source cluster-computing framework.
    <br />
    DataLabs supports usage of Spark via Kubernetes which allows Spark executors to be deployed across the cluster.
  </WithStyles(ForwardRef(Typography))>
  <div
    className="makeStyles-clusterList-1"
  >
    <ClustersContainer
      clusterType="SPARK"
      copySnippets={
        Object {
          "Python": [Function],
          "R": [Function],
        }
      }
    />
  </div>
</WithStyles(Page)>
`;

exports[`getPythonMessage returns the correct message 1`] = `
"import os
import pyspark

conf = pyspark.SparkConf()
# The below option can be altered depending on your memory requirement.
# The maximum value is the amount of memory assigned to each worker, minus 1GB.
conf.set('spark.executor.memory', '3g')

os.environ[\\"PYSPARK_PYTHON\\"] = \\"/data/conda/myenv/bin/python\\"
os.environ[\\"PYSPARK_DRIVER_PYTHON\\"] = \\"/data/conda/myenv/bin/python\\"

sc = pyspark.SparkContext(master=\\"spark://spark-scheduler-mycluster:7077\\", conf=conf)
sc
"
`;

exports[`getPythonMessage returns the correct message if memory is not an integer 1`] = `
"import os
import pyspark

conf = pyspark.SparkConf()
# The below option can be altered depending on your memory requirement.
# The maximum value is the amount of memory assigned to each worker, minus 1GB.
conf.set('spark.executor.memory', '4g')

os.environ[\\"PYSPARK_PYTHON\\"] = \\"/data/conda/myenv/bin/python\\"
os.environ[\\"PYSPARK_DRIVER_PYTHON\\"] = \\"/data/conda/myenv/bin/python\\"

sc = pyspark.SparkContext(master=\\"spark://spark-scheduler-mycluster:7077\\", conf=conf)
sc
"
`;

exports[`getRMessage returns the correct message 1`] = `
"library(SparkR)

# The below option can be altered depending on your memory requirement.
# The maximum value is the amount of memory assigned to each worker, minus 1GB.
sparkR.session(master=\\"spark://spark-scheduler-mycluster:7077\\", sparkConfig=list(spark.executor.memory=\\"3g\\"))"
`;

exports[`getRMessage returns the correct message if memory is not an integer 1`] = `
"library(SparkR)

# The below option can be altered depending on your memory requirement.
# The maximum value is the amount of memory assigned to each worker, minus 1GB.
sparkR.session(master=\\"spark://spark-scheduler-mycluster:7077\\", sparkConfig=list(spark.executor.memory=\\"4g\\"))"
`;
