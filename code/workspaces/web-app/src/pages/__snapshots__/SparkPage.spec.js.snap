// Jest Snapshot v1, https://goo.gl/fbAQLP

exports[`SparkPage renders correct snapshot 1`] = `
<div>
  <div
    class="Page-pageTemplate-2"
  >
    <h4
      class="MuiTypography-root Page-title-4 MuiTypography-h4"
    >
      Spark
    </h4>
    <div
      class="Page-contentArea-3"
    >
      <p
        class="MuiTypography-root MuiTypography-body1"
      >
        <a
          href="https://spark.apache.org/"
          rel="noopener noreferrer"
          target="_blank"
        >
          Apache Spark
        </a>
         is an open-source cluster-computing framework.
        <br />
        DataLabs supports usage of Spark via Kubernetes which allows Spark executors to be deployed across the cluster.
      </p>
      <div
        class="makeStyles-clusterList-1"
      >
        <div>
          ClustersContainer mock 
          {"clusterType":"SPARK","copySnippets":{}}
        </div>
      </div>
    </div>
    <div>
      <hr
        class="MuiDivider-root Footer-divider-35"
      />
      <p
        class="MuiTypography-root Footer-versionText-36 MuiTypography-body2"
      >
        Version: undefined
      </p>
    </div>
  </div>
</div>
`;

exports[`getPythonMessage returns the correct message 1`] = `
"import os
import pyspark

conf = pyspark.SparkConf()
# The below option can be altered depending on your memory requirement.
# The maximum value is the amount of memory assigned to each worker, minus 1GB.
conf.set('spark.executor.memory', '3g')

os.environ[\\"PYSPARK_PYTHON\\"] = \\"/data/conda/myenv/bin/python\\"
os.environ[\\"PYSPARK_DRIVER_PYTHON\\"] = \\"/data/conda/myenv/bin/python\\"

sc = pyspark.SparkContext(master=\\"spark://spark-scheduler-mycluster:7077\\", conf=conf)
sc
"
`;

exports[`getPythonMessage returns the correct message if memory is not an integer 1`] = `
"import os
import pyspark

conf = pyspark.SparkConf()
# The below option can be altered depending on your memory requirement.
# The maximum value is the amount of memory assigned to each worker, minus 1GB.
conf.set('spark.executor.memory', '4g')

os.environ[\\"PYSPARK_PYTHON\\"] = \\"/data/conda/myenv/bin/python\\"
os.environ[\\"PYSPARK_DRIVER_PYTHON\\"] = \\"/data/conda/myenv/bin/python\\"

sc = pyspark.SparkContext(master=\\"spark://spark-scheduler-mycluster:7077\\", conf=conf)
sc
"
`;

exports[`getRMessage returns the correct message 1`] = `
"library(SparkR)

# The below option can be altered depending on your memory requirement.
# The maximum value is the amount of memory assigned to each worker, minus 1GB.
sparkR.session(master=\\"spark://spark-scheduler-mycluster:7077\\", sparkConfig=list(spark.executor.memory=\\"3g\\"))"
`;

exports[`getRMessage returns the correct message if memory is not an integer 1`] = `
"library(SparkR)

# The below option can be altered depending on your memory requirement.
# The maximum value is the amount of memory assigned to each worker, minus 1GB.
sparkR.session(master=\\"spark://spark-scheduler-mycluster:7077\\", sparkConfig=list(spark.executor.memory=\\"4g\\"))"
`;
