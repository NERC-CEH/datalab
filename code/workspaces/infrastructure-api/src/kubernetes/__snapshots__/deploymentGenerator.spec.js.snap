// Jest Snapshot v1, https://goo.gl/fbAQLP

exports[`deploymentGenerator createDatalabDaskSchedulerDeployment uses dask image if no volumeMount/condaPath set 1`] = `
"apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-name
spec:
  replicas: 1
  selector:
    matchLabels:
      dask-infra: scheduler-pod-label
  template:
    metadata:
      labels:
        dask-infra: scheduler-pod-label
        name: DASK-deployment-name
        user-pod: dask
    spec:
      containers:
        - image: dask-image
          command: [\\"bash\\"]
          args: [\\"-c\\", \\"dask-scheduler\\"]
          name: dask-scheduler-cont
          ports:
            - containerPort: 8786
            - containerPort: 8787
          resources:
            requests:
              memory: scheduler-memory
              cpu: scheduler-cpu
            limits:
              memory: scheduler-memory
              cpu: scheduler-cpu
          volumeMounts:
      hostname: dask-scheduler-host
      restartPolicy: Always
      volumes:
"
`;

exports[`deploymentGenerator createDatalabDaskSchedulerDeployment uses jupyter image if volumeMount/condaPath set 1`] = `
"apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-name
spec:
  replicas: 1
  selector:
    matchLabels:
      dask-infra: scheduler-pod-label
  template:
    metadata:
      labels:
        dask-infra: scheduler-pod-label
        name: DASK-deployment-name
        user-pod: dask
    spec:
      containers:
        - image: lab-image
          command: [\\"bash\\"]
          args: [\\"-c\\", \\"/data/conda/some-env/bin/dask-scheduler\\"]
          name: dask-scheduler-cont
          ports:
            - containerPort: 8786
            - containerPort: 8787
          resources:
            requests:
              memory: scheduler-memory
              cpu: scheduler-cpu
            limits:
              memory: scheduler-memory
              cpu: scheduler-cpu
          volumeMounts:
            - name: persistentfsvol
              mountPath: /data
      hostname: dask-scheduler-host
      restartPolicy: Always
      volumes:
        - name: persistentfsvol
          persistentVolumeClaim:
            claimName: volume-mount-claim
"
`;

exports[`deploymentGenerator createDatalabDaskWorkerDeployment uses dask image if no volumeMount/condaPath set 1`] = `
"apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-name
spec:
  replicas: 1
  selector:
    matchLabels:
      dask-infra: worker-pod-label
  template:
    metadata:
      labels:
        dask-infra: worker-pod-label
    spec:
      containers:
        - image: dask-image
          command: [\\"bash\\"]
          args: [\\"-c\\", \\"dask-worker --nthreads n-threads --no-dashboard --memory-limit worker-memory --death-timeout death-timeout-sec tcp://scheduler-service-name:8786\\"]
          name: dask-worker-cont
          resources:
            requests:
              memory: worker-memory
              cpu: worker-cpu
            limits:
              memory: worker-memory
              cpu: worker-cpu
          volumeMounts:
      hostname: dask-worker-host
      restartPolicy: Always
      volumes:
"
`;

exports[`deploymentGenerator createDatalabDaskWorkerDeployment uses jupyter image if volumeMount/condaPath set 1`] = `
"apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-name
spec:
  replicas: 1
  selector:
    matchLabels:
      dask-infra: worker-pod-label
  template:
    metadata:
      labels:
        dask-infra: worker-pod-label
    spec:
      containers:
        - image: lab-image
          command: [\\"bash\\"]
          args: [\\"-c\\", \\"/data/conda/some-env/bin/dask-worker --nthreads n-threads --no-dashboard --memory-limit worker-memory --death-timeout death-timeout-sec tcp://scheduler-service-name:8786\\"]
          name: dask-worker-cont
          resources:
            requests:
              memory: worker-memory
              cpu: worker-cpu
            limits:
              memory: worker-memory
              cpu: worker-cpu
          volumeMounts:
            - name: persistentfsvol
              mountPath: /data
      hostname: dask-worker-host
      restartPolicy: Always
      volumes:
        - name: persistentfsvol
          persistentVolumeClaim:
            claimName: volume-mount-claim
"
`;

exports[`deploymentGenerator createDatalabSparkSchedulerDeployment uses spark image 1`] = `
"apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-name
spec:
  replicas: 1
  selector:
    matchLabels:
      spark-infra: scheduler-pod-label
  template:
    metadata:
      labels:
        spark-infra: scheduler-pod-label
        name: SPARK-deployment-name
        user-pod: spark
    spec:
      containers:
        - image: spark-image
          name: spark-scheduler-cont
          env:
            - name: SPARK_MODE
              value: \\"master\\"
          ports:
            - containerPort: 8080
            - containerPort: 7077
          resources:
            requests:
              memory: scheduler-memory
              cpu: scheduler-cpu
            limits:
              memory: scheduler-memory
              cpu: scheduler-cpu
          volumeMounts:
      hostname: spark-scheduler-host
      restartPolicy: Always
      volumes:
"
`;

exports[`deploymentGenerator createDatalabSparkWorkerDeployment uses spark image 1`] = `
"apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-name
spec:
  replicas: 1
  selector:
    matchLabels:
      spark-infra: worker-pod-label
  template:
    metadata:
      labels:
        spark-infra: worker-pod-label
    spec:
      containers:
        - image: spark-image
          name: spark-worker-cont
          env:
            - name: SPARK_MODE
              value: \\"worker\\"
            - name: SPARK_MASTER_URL
              value: \\"spark://scheduler-service-name:7077\\"
          resources:
            requests:
              memory: worker-memory
              cpu: worker-cpu
            limits:
              memory: worker-memory
              cpu: worker-cpu
          volumeMounts:
      hostname: spark-worker-host
      restartPolicy: Always
      volumes:
"
`;

exports[`deploymentGenerator createJupyterDeployment generates Jupyter Notebook manifest 1`] = `
"---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    name: deployment-name
  name: deployment-name
spec:
  selector:
    matchLabels:
      name: deployment-name
  template:
    metadata:
      labels:
        name: deployment-name
        user-pod: jupyter
    spec:
      securityContext:
        runAsUser: 0
      serviceAccountName: project-key-compute-submission-account
      initContainers:
        # This container will generate the expected folder structure for the notebooks directory, without this the
        # jupyter container will not start. As the container runs a root these directories are owned by root and can
        # not be written to by datalabs:users (user:group; 1000:100).
        - name: create-notebook-folders
          image: busybox
          imagePullPolicy: IfNotPresent
          command: [\\"sh\\"]
          args: [\\"-c\\", \\"mkdir -p /mnt/persistentfs/notebooks/deployment-name\\"]
          volumeMounts:
            - mountPath: /mnt/persistentfs
              name: persistentfsvol
        # This container sets the folder permissions for the generated notebook directory to be usable by
        # datalabs:users.
        - name: set-directory-permissions
          image: busybox
          imagePullPolicy: IfNotPresent
          command: [\\"sh\\"]
          args: [\\"-c\\", \\"chmod 777 /mnt/persistentfs/notebooks && chmod 777 /mnt/persistentfs/notebooks/deployment-name && ln -sfn /assets /mnt/persistentfs/notebooks/deployment-name/assets\\"]
          volumeMounts:
            - mountPath: /mnt/persistentfs
              name: persistentfsvol
      containers:
        - image: nerc/jupyterlab:ee1e157f36f5-spark-3.3.0
          imagePullPolicy : \\"IfNotPresent\\"
          name: deployment-name
          command: [\\"start.sh\\",
            \\"jupyter\\", \\"notebook\\",
            \\"--NotebookApp.token=$(JUPYTER_TOKEN)\\",
            \\"--NotebookApp.allow_origin=project-key-notebook-name.datalabs.localhost\\",
            \\"--NotebookApp.notebook_dir=/data/notebooks/deployment-name\\",
            \\"--NotebookApp.base_url=/resource/project-key/notebook-name\\"          ]
          ports:
            - containerPort: 8888
              protocol: TCP
          env:
            - name: JUPYTER_TOKEN
              valueFrom:
                secretKeyRef:
                  name: deployment-name
                  key: token
            - name: GRANT_SUDO
              value: \\"yes\\"
            - name: R_LIBS_USER
              value: \\"/data/packages/R/%p/%v\\"
            - name: JUPYTER_DATA_DIR
              value: \\"/data/.jupyter\\"
            - name: CONDA_ENV_DIR
              value: \\"/data/conda/\\"
            - name: JUPYTER_ALLOW_INSECURE_WRITES
              value: 'true'
          resources:
            requests:
              cpu: 200m
              memory: 128Mi
            limits:
              cpu: 2
              memory: 8Gi
          livenessProbe:
            httpGet:
              path: /resource/project-key/notebook-name
              port: 8888
            initialDelaySeconds: 5
            periodSeconds: 10
          volumeMounts:
            - name: persistentfsvol
              mountPath: /data
            - name: spark-kubernetes-properties
              # /usr/local/spark is default value of $SPARK_HOME set in the
              # base jupyter/pyspark-notebook Dockerfile
              mountPath: /usr/local/spark/conf/spark-defaults.conf
              subPath: spark-defaults.conf
            - name: dask-kubernetes-properties
              mountPath: /opt/conda/etc/dask/dask.yaml
              subPath: dask.yaml
            - name: jupyter-notebook-config
              mountPath: /home/datalab/.jupyter/jupyter_notebook_config.py
              subPath: jupyter-notebook-config.py
      volumes:
        - name: persistentfsvol
          persistentVolumeClaim:
            claimName: volume-mount-claim
        - name: spark-kubernetes-properties
          configMap:
            name: deployment-name-pyspark-config
            items:
              - key: config
                path: spark-defaults.conf
        - name: dask-kubernetes-properties
          configMap:
            name: deployment-name-dask-config
            items:
            - key: config
              path: dask.yaml
        - name: jupyter-notebook-config
          configMap:
            name: deployment-name-jupyter-config
            items:
            - key: config
              path: jupyter-notebook-config.py
"
`;

exports[`deploymentGenerator createJupyterDeployment generates JupyterLab manifest 1`] = `
"---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    name: deployment-name
  name: deployment-name
spec:
  selector:
    matchLabels:
      name: deployment-name
  template:
    metadata:
      labels:
        name: deployment-name
        user-pod: jupyterlab
    spec:
      securityContext:
        runAsUser: 0
      serviceAccountName: project-key-compute-submission-account
      initContainers:
        # This container will generate the expected folder structure for the notebooks directory, without this the
        # jupyter container will not start. As the container runs a root these directories are owned by root and can
        # not be written to by datalabs:users (user:group; 1000:100).
        - name: create-notebook-folders
          image: busybox
          imagePullPolicy: IfNotPresent
          command: [\\"sh\\"]
          args: [\\"-c\\", \\"mkdir -p /mnt/persistentfs/notebooks/deployment-name\\"]
          volumeMounts:
            - mountPath: /mnt/persistentfs
              name: persistentfsvol
        # This container sets the folder permissions for the generated notebook directory to be usable by
        # datalabs:users.
        - name: set-directory-permissions
          image: busybox
          imagePullPolicy: IfNotPresent
          command: [\\"sh\\"]
          args: [\\"-c\\", \\"chmod 777 /mnt/persistentfs/notebooks && chmod 777 /mnt/persistentfs/notebooks/deployment-name && ln -sfn /assets /mnt/persistentfs/notebooks/deployment-name/assets\\"]
          volumeMounts:
            - mountPath: /mnt/persistentfs
              name: persistentfsvol
      containers:
        - image: nerc/jupyterlab:ee1e157f36f5-spark-3.3.0
          imagePullPolicy : \\"IfNotPresent\\"
          name: deployment-name
          command: [\\"start.sh\\",
            \\"jupyter\\", \\"lab\\",
            \\"--NotebookApp.token=$(JUPYTER_TOKEN)\\",
            \\"--NotebookApp.allow_origin=project-key-notebook-name.datalabs.localhost\\",
            \\"--NotebookApp.notebook_dir=/data/notebooks/deployment-name\\",
            \\"--NotebookApp.base_url=/resource/project-key/notebook-name\\",
            \\"--collaborative\\"
          ]
          ports:
            - containerPort: 8888
              protocol: TCP
          env:
            - name: JUPYTER_TOKEN
              valueFrom:
                secretKeyRef:
                  name: deployment-name
                  key: token
            - name: GRANT_SUDO
              value: \\"yes\\"
            - name: R_LIBS_USER
              value: \\"/data/packages/R/%p/%v\\"
            - name: JUPYTER_DATA_DIR
              value: \\"/data/.jupyter\\"
            - name: CONDA_ENV_DIR
              value: \\"/data/conda/\\"
            - name: JUPYTER_ALLOW_INSECURE_WRITES
              value: 'true'
          resources:
            requests:
              cpu: 200m
              memory: 128Mi
            limits:
              cpu: 2
              memory: 8Gi
          livenessProbe:
            httpGet:
              path: /resource/project-key/notebook-name
              port: 8888
            initialDelaySeconds: 5
            periodSeconds: 10
          volumeMounts:
            - name: persistentfsvol
              mountPath: /data
            - name: spark-kubernetes-properties
              # /usr/local/spark is default value of $SPARK_HOME set in the
              # base jupyter/pyspark-notebook Dockerfile
              mountPath: /usr/local/spark/conf/spark-defaults.conf
              subPath: spark-defaults.conf
            - name: dask-kubernetes-properties
              mountPath: /opt/conda/etc/dask/dask.yaml
              subPath: dask.yaml
            - name: jupyter-notebook-config
              mountPath: /home/datalab/.jupyter/jupyter_notebook_config.py
              subPath: jupyter-notebook-config.py
      volumes:
        - name: persistentfsvol
          persistentVolumeClaim:
            claimName: volume-mount-claim
        - name: spark-kubernetes-properties
          configMap:
            name: deployment-name-pyspark-config
            items:
              - key: config
                path: spark-defaults.conf
        - name: dask-kubernetes-properties
          configMap:
            name: deployment-name-dask-config
            items:
            - key: config
              path: dask.yaml
        - name: jupyter-notebook-config
          configMap:
            name: deployment-name-jupyter-config
            items:
            - key: config
              path: jupyter-notebook-config.py
"
`;

exports[`deploymentGenerator createRStudioConfigMap produces expected configmap 1`] = `
"---
apiVersion: v1
kind: ConfigMap
metadata:
  name: configmap-name
data:
  X-RStudio-Root-Path: /base/path
"
`;

exports[`deploymentGenerator createRStudioDeployment creates expected manifest 1`] = `
"---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    name: rstudio-name
  name: rstudio-name
spec:
  selector:
    matchLabels:
      name: rstudio-name
  template:
    metadata:
      labels:
        name: rstudio-name
        user-pod: rstudio
    spec:
      initContainers:
        # This container will generate the RStudio notebooks directory. RStudio starts with /home/datalabs as its initial
        # start-up/default directory and has not environmental variable to change it. This is fixed by mounting an empty
        # directory as /home and created a symlink datalabs pointing to /data/notebooks/rstudio
        - name: create-notebook-folders
          image: busybox
          imagePullPolicy: IfNotPresent
          command: [\\"sh\\"]
          args: [\\"-c\\", \\"mkdir -p /data/notebooks/rstudio-name\\"]
          volumeMounts:
            - mountPath: /data
              name: persistentfsvol
        # This container sets the folder permissions for the generated notebook directory to be usable by
        # datalabs:users.
        - name: set-directory-permissions
          image: busybox
          imagePullPolicy: IfNotPresent
          command: [\\"sh\\"]
          args: [\\"-c\\", \\"chmod 777 /data/notebooks && chmod 777 /data/notebooks/rstudio-name && ln -sfn /assets /data/notebooks/rstudio-name/assets\\"]
          volumeMounts:
            - mountPath: /data
              name: persistentfsvol
        # This container will create the /home/datalabs to /data/notebooks/rstudio symlink
        - name: create-datalab-symlink
          image: busybox
          imagePullPolicy: IfNotPresent
          command: [\\"sh\\", \\"-c\\", \\"ln -s /data/notebooks/rstudio-name/ /home/datalab\\"]
          volumeMounts:
            - mountPath: /home
              name: homedir
      containers:
        - name: rstudio-name-connect
          image: nerc/zeppelin-connect:1.2.0
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8000
              protocol: TCP
          env:
            - name: CONNECT_TYPE
              value: RSTUDIO
          livenessProbe:
            httpGet:
              path: /status
              port: 8000
            initialDelaySeconds: 5
            periodSeconds: 10
        - name: rstudio-name
          image: nerc/rstudio:4.0.1
          imagePullPolicy : IfNotPresent
          ports:
            - containerPort: 8787
              protocol: TCP
          env:
            - name: USER
              value: datalab
            - name: PASSWORD
              valueFrom:
                secretKeyRef:
                  name: rstudio-name
                  key: password
            - name: ROOT
              value: 'TRUE'
          resources:
            requests:
              cpu: 200m
              memory: 128Mi
            limits:
              cpu: 2
              memory: 8Gi
          livenessProbe:
            httpGet:
              path: /
              port: 8787
            initialDelaySeconds: 5
            periodSeconds: 10
          volumeMounts:
            - name: persistentfsvol
              mountPath: /data
            - name: homedir
              mountPath: /home
      volumes:
        - name: persistentfsvol
          persistentVolumeClaim:
            claimName: volumeMount-claim
        - name: homedir
          emptyDir: {}
"
`;

exports[`deploymentGenerator createSiteDeployment creates expected manifest 1`] = `
"---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    name: panel-name
  name: panel-name
spec:
  selector:
    matchLabels:
      name: panel-name
  template:
    metadata:
      labels:
        name: panel-name
        user-pod: panel
    spec:
      # No init container need as the subPath variable only mounts the required directory. If the directory is missing
      # this is automatically created.
      containers:
        - name: panel-name
          image: nerc/jupyterlab:ee1e157f36f5-spark-3.3.0
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 5006
              protocol: TCP
          command: [\\"/bin/bash\\"]
          args: [\\"-c\\",
            \\"cd /notebooks;
            if [ -n \\\\\\"$CONDA_ENV\\\\\\" ]; then
              source activate $CONDA_ENV;
            fi;
            if [ -n \\\\\\"$FILENAME\\\\\\" ]; then
              panel serve $FILENAME;
            else 
              panel serve *.ipynb *.py --glob;
            fi;\\"]
          env:
            - name: CONDA_ENV_DIR
              value: \\"/data/conda/\\"
            - name: JUPYTER_DATA_DIR
              value: \\"/data/.jupyter\\"
            - name: CONDA_ENV
              value: /data/conda/my-env
            - name: FILENAME
              value: notebook.ipynb
            - name: BOKEH_ALLOW_WS_ORIGIN
              value: localhost:5006,project-panel.datalabs.nerc.ac.uk
          livenessProbe:
            httpGet:
              path: /
              port: 5006
            initialDelaySeconds: 5
            periodSeconds: 10
          volumeMounts:
            - name: persistentfsvol
              mountPath: /notebooks
              subPath: notebooks/my-notebook
              readOnly: true
            - name: persistentfsvol
              mountPath: /data/conda
              subPath: conda
              readOnly: true
            - name: persistentfsvol
              mountPath: /data/.jupyter
              subPath: .jupyter
              readOnly: true
      volumes:
        - name: persistentfsvol
          persistentVolumeClaim:
            claimName: volumeMount-claim
"
`;

exports[`deploymentGenerator createSiteDeployment creates expected manifest for RShiny 1`] = `
"---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    name: rshiny-name
  name: rshiny-name
spec:
  replicas: 1
  selector:
    matchLabels:
      name: rshiny-name
  template:
    metadata:
      labels:
        name: rshiny-name
        user-pod: rshiny
    spec:
      containers:
        - image: nerc/rshiny:4.0.1
          imagePullPolicy: \\"IfNotPresent\\"
          name: rshiny-name
          ports:
            - containerPort: 3838
              protocol: TCP
          resources:
            requests:
              cpu: 200m
              memory: 128Mi
            limits:
              cpu: 1
              memory: 2Gi
          # Setting these custom values based on loadup times using packrat and
          # experience with single threaded RShiny applications which can be
          # slow to respond to polling (default timeout = 1 second)
          livenessProbe:
            httpGet:
              path: /
              port: 3838
            initialDelaySeconds: 30
            periodSeconds: 60
            timeoutSeconds: 10
          volumeMounts:
            - name: glusterfsvol
              mountPath: /data
              readOnly: true
            - name: glusterfsvol
              mountPath: /srv/shiny-server
              subPath: notebooks/my-notebook
      volumes:
        - name: glusterfsvol
          persistentVolumeClaim:
            claimName: volumeMount-claim
"
`;

exports[`deploymentGenerator createSiteService creates expected manifest 1`] = `
"---
apiVersion: v1
kind: Service
metadata:
  name: panel-name
spec:
  ports:
  - name: panel-name-web-ui
    port: 80
    targetPort: 5006
  selector:
    name: panel-name
  type: NodePort
"
`;

exports[`deploymentGenerator createSiteService creates expected manifest for RShiny 1`] = `
"---
kind: Service
apiVersion: v1
metadata:
  name: rshiny-name
spec:
  ports:
    - name: rshiny-name-web-ui
      port: 80
      targetPort: 3838
  selector:
    name: rshiny-name
  type: NodePort
"
`;

exports[`deploymentGenerator createVSCodeDeployment generates Jupyter Notebook manifest 1`] = `
"---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    name: deployment-name
  name: deployment-name
spec:
  selector:
    matchLabels:
      name: deployment-name
  template:
    metadata:
      labels:
        name: deployment-name
        user-pod: vscode
    spec:
      securityContext:
        runAsUser: 1000
      serviceAccountName: project-key-compute-submission-account
      initContainers:
        # This container will generate the expected folder structure, which has a workspace folder for vscode and conda folder for conda.
        # As the container runs a root these directories are owned by root and can
        # not be written to by datalabs:users (user:group; 1000:100).
        - name: create-notebook-folders
          image: busybox
          imagePullPolicy: IfNotPresent
          command: [\\"bash\\"]
          args: [\\"-c\\", \\"mkdir -p /mnt/persistentfs/notebooks/deployment-name/{workspace,conda} /mnt/persistentfs/notebooks/deployment-name/.vscode/{extensions,data}\\"]
          volumeMounts:
            - mountPath: /mnt/persistentfs
              name: persistentfsvol
        # This container sets the folder permissions for the generated workspace and conda directories to be usable by
        # datalabs:users.
        - name: set-directory-permissions
          image: busybox
          imagePullPolicy: IfNotPresent
          command: [\\"bash\\"]
          args: [\\"-c\\", \\"chmod 777 /mnt/persistentfs/notebooks/deployment-name /mnt/persistentfs/notebooks/deployment-name/workspace /mnt/persistentfs/notebooks/deployment-name/conda && ln -sfn /assets /mnt/persistentfs/notebooks/deployment-name/assets\\"]
          volumeMounts:
            - mountPath: /mnt/persistentfs
              name: persistentfsvol
      containers:
        - image: nerc/vscode-server:fb79ef9019894b9129ff5083eba6388574e737b7-0.0.1
          imagePullPolicy : \\"IfNotPresent\\"
          name: deployment-name
          command: [\\"/bin/sh\\",
            \\"-c\\",
            \\"exec /home/.openvscode-server/bin/openvscode-server --host 0.0.0.0 --connection-token $(ACCESS_TOKEN) --user-data-dir /data/notebooks/deployment-name/.vscode/data  --extensions-dir
          /data/notebooks/deployment-name/.vscode/extensions\\"
          ]
          ports:
            - containerPort: 3000
              protocol: TCP
          env:
            - name: ACCESS_TOKEN
              valueFrom:
                secretKeyRef:
                  name: deployment-name
                  key: token
            - name: GRANT_SUDO
              value: \\"yes\\"
            - name: CONDA_ENV_DIR
              value: \\"/data/conda/\\"
          resources:
            requests:
              cpu: 200m
              memory: 128Mi
            limits:
              cpu: 2
              memory: 8Gi
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 3000
            timeoutSeconds: 5
          volumeMounts:
            - name: persistentfsvol
              mountPath: /data
      volumes:
        - name: persistentfsvol
          persistentVolumeClaim:
            claimName: volume-mount-claim
"
`;

exports[`deploymentGenerator createVSCodeDeployment generates VSCode manifest 1`] = `
"---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    name: deployment-name
  name: deployment-name
spec:
  selector:
    matchLabels:
      name: deployment-name
  template:
    metadata:
      labels:
        name: deployment-name
        user-pod: vscode
    spec:
      securityContext:
        runAsUser: 1000
      serviceAccountName: project-key-compute-submission-account
      initContainers:
        # This container will generate the expected folder structure, which has a workspace folder for vscode and conda folder for conda.
        # As the container runs a root these directories are owned by root and can
        # not be written to by datalabs:users (user:group; 1000:100).
        - name: create-notebook-folders
          image: busybox
          imagePullPolicy: IfNotPresent
          command: [\\"bash\\"]
          args: [\\"-c\\", \\"mkdir -p /mnt/persistentfs/notebooks/deployment-name/{workspace,conda} /mnt/persistentfs/notebooks/deployment-name/.vscode/{extensions,data}\\"]
          volumeMounts:
            - mountPath: /mnt/persistentfs
              name: persistentfsvol
        # This container sets the folder permissions for the generated workspace and conda directories to be usable by
        # datalabs:users.
        - name: set-directory-permissions
          image: busybox
          imagePullPolicy: IfNotPresent
          command: [\\"bash\\"]
          args: [\\"-c\\", \\"chmod 777 /mnt/persistentfs/notebooks/deployment-name /mnt/persistentfs/notebooks/deployment-name/workspace /mnt/persistentfs/notebooks/deployment-name/conda && ln -sfn /assets /mnt/persistentfs/notebooks/deployment-name/assets\\"]
          volumeMounts:
            - mountPath: /mnt/persistentfs
              name: persistentfsvol
      containers:
        - image: nerc/vscode-server:fb79ef9019894b9129ff5083eba6388574e737b7-0.0.1
          imagePullPolicy : \\"IfNotPresent\\"
          name: deployment-name
          command: [\\"/bin/sh\\",
            \\"-c\\",
            \\"exec /home/.openvscode-server/bin/openvscode-server --host 0.0.0.0 --connection-token $(ACCESS_TOKEN) --user-data-dir /data/notebooks/deployment-name/.vscode/data  --extensions-dir
          /data/notebooks/deployment-name/.vscode/extensions\\"
          ]
          ports:
            - containerPort: 3000
              protocol: TCP
          env:
            - name: ACCESS_TOKEN
              valueFrom:
                secretKeyRef:
                  name: deployment-name
                  key: token
            - name: GRANT_SUDO
              value: \\"yes\\"
            - name: CONDA_ENV_DIR
              value: \\"/data/conda/\\"
          resources:
            requests:
              cpu: 200m
              memory: 128Mi
            limits:
              cpu: 2
              memory: 8Gi
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 3000
            timeoutSeconds: 5
          volumeMounts:
            - name: persistentfsvol
              mountPath: /data
      volumes:
        - name: persistentfsvol
          persistentVolumeClaim:
            claimName: volume-mount-claim
"
`;

exports[`deploymentGenerator generates createAutoScaler manifest 1`] = `
"apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: auto-scaler-name
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: scale-deployment-name
  minReplicas: 1
  maxReplicas: max-replicas
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: target-cpu-utilization
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: target-memory-utilization
  behavior:
    scaleDown:
      stabilizationWindowSeconds: scale-down-window-sec
"
`;

exports[`deploymentGenerator generates createDatalabDaskSchedulerNetworkPolicy manifest 1`] = `
"apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: policy-name
spec:
  podSelector:
    matchLabels:
      dask-infra: scheduler-pod-label
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: project-key
    ports:
    - protocol: TCP
      port: 8786
    - protocol: TCP
      port: 8787
"
`;

exports[`deploymentGenerator generates createDatalabDaskSchedulerService manifest 1`] = `
"apiVersion: v1
kind: Service
metadata:
  name: service-name
spec:
  ports:
    - name: \\"client\\"
      port: 8786
      targetPort: 8786
    - name: \\"dashboard\\"
      port: 8787
      targetPort: 8787
  selector:
    dask-infra: scheduler-pod-label
"
`;

exports[`deploymentGenerator generates createDatalabSparkSchedulerNetworkPolicy manifest 1`] = `
"apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: policy-name
spec:
  podSelector:
    matchLabels:
      spark-infra: scheduler-pod-label
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: project-key
      ports:
        - protocol: TCP
          port: 8080
        - protocol: TCP
          port: 7077
"
`;

exports[`deploymentGenerator generates createDatalabSparkSchedulerService manifest 1`] = `
"apiVersion: v1
kind: Service
metadata:
  name: service-name
spec:
  ports:
    - name: \\"client\\"
      port: 8080
      targetPort: 8080
    - name: \\"driver\\"
      port: 7077
      targetPort: 7077
  selector:
    spark-infra: scheduler-pod-label
"
`;
