# Lighthouse Results

_**Note**: this README is autogenerated. Do not edit directly. See [Generating README](#generating-readme) below on how to make changes._

[Lighthouse](https://developers.google.com/web/tools/lighthouse/#report-viewer) is an open-source tool from Google that analyses webpages and gives them a score out of 100 in a number of key areas e.g. performance and accessibility.
This directory contains code to run Lighthouse in an automated way across the different pages in DataLabs.
See [Running Lighthouse tests](#running-lighthouse-tests) for more information.

## Summary of results

The following table shows Lighthouse results obtained for each of the pages currently available in the web-app.
All endpoints were tested against https://testlab.test-datalabs.nerc.ac.uk unless given otherwise.

| Endpoint                    | Report File                              | Accessibility (%) | Performance (%) | Best Practices (%) | SEO (%) |
| --------------------------- | ---------------------------------------- | :---------------: | :-------------: | :----------------: | :-----: |
| /                           | lhreport-.json                           |        97         |        0        |        100         |   82    |
| /404                        | lhreport-404.json                        |        91         |       50        |        100         |   73    |
| /admin/resources            | lhreport-admin-resources.json            |        70         |        0        |         93         |   73    |
| /projects                   | lhreport-projects.json                   |        77         |        0        |        100         |   73    |
| /projects/lhouse/dask       | lhreport-projects-lhouse-dask.json       |        87         |       66        |        100         |   73    |
| /projects/lhouse/info       | lhreport-projects-lhouse-info.json       |        88         |        0        |        100         |   73    |
| /projects/lhouse/notebooks  | lhreport-projects-lhouse-notebooks.json  |        80         |        0        |        100         |   73    |
| /projects/lhouse/publishing | lhreport-projects-lhouse-publishing.json |        80         |        0        |        100         |   73    |
| /projects/lhouse/settings   | lhreport-projects-lhouse-settings.json   |        77         |        0        |        100         |   73    |
| /projects/lhouse/spark      | lhreport-projects-lhouse-spark.json      |        87         |       53        |        100         |   73    |
| /projects/lhouse/storage    | lhreport-projects-lhouse-storage.json    |        87         |       53        |        100         |   73    |
| /verify                     | lhreport-verify.json                     |        95         |        0        |        100         |   82    |

### Viewing more detailed results

The summary of results is generated from the JSON output of Lighthouse (stored in the `reports` directory).
These JSON files contain more information than is displayed in the summary table above, but they aren't the most human readable.
To view this extra detail, the contents of the JSON file can be dropped onto the [Lighthouse Report Viewer](https://googlechrome.github.io/lighthouse/viewer/) which renders a web-page built from the contents of the JSON file.

## Running Lighthouse tests

> Note: The lighthouse tests now require a newer version of node than the services so you must run an updated version. This is easiest to do with `nvm` and allows you to switch versions when specifically running the lighthouse tests until we can upgrade the whole system. The latest run of the lighthouse tests used node v16.13.0.

There is a script `lighthouse-testing.js` that handles running the Lighthouse tests.
This uses the `lighthouse` NPM module as well as a test orchestration library called `puppetteer`.
At the top of this script a number of constants are defined that can be used to configure the tests being run.
Two of these constants defined the endpoints on which the tests are to be run: one for insecure, publicly accessible endpoints and for secure endpoints that require being logged in.
When the tests are run, a chromium window will open that will navigate to each of the endpoints that are to be tested.
All of this is automated.

In order to be able to run the tests, you will need to provide the script with a username and password that it can use to log into the system with.
These credentials are read from the `LIGHTHOUSE_TEST_USER_NAME` and `LIGHTHOUSE_TEST_PASSWORD` environment variables respectively.
The script should fail before trying to run any of the tests if it detects that either of these environement variables are not set.

To run the tests you will need to have access to a node executable and Yarn.
Before running the tests for the first time you will need to run `yarn install` to install the necessary modules.
The tests can then be run using `node lighthouse-testing.js`.

To securely run the lighthouse tests and avoid putting your email password in to the history, you can add the password to a `password.txt` file in this directory (which will be ignored by git). You can then run the tests using:

```
LIGHTHOUSE_TEST_PASSWORD="$(cat password.txt)" LIGHTHOUSE_TEST_USER_NAME={{insert your email here}} node lighthouse-testing.js
```

Once the Lighthouse tests have been run, the results are written in JSON format to the `reports` directory.
Following this, the script used to generate this README is executed (see [Generating README](#generating-readme)).

### Configuring project to run tests against

By default, the tests are configured to run against the deployed test instance of DataLabs and run against a project specifically created for the purpose.
This project is called [Lighthouse Testing (`LHOUSE`)](https://testlab.test-datalabs.nerc.ac.uk/projects/lhouse/info) and each member of the development team should be an admin on the project.
Secondary accounts have been added to the project such that there is at least one user in each role on the project.

To ensure that as much of the UI is covered in the testing as possible, there should be a certain number of resources in the project. These are as follows:

- 11 notebooks:
  - More than one to ensure that there are no issues around unique keys etc.
  - Creates another page of notebooks to display the pagination component so this can be tested.
  - At least one of these should be shared in the project to display the additional text.
- 2 sites:
  - More than one to ensure that there are no issues around unique keys etc.
  - At least one of these should be shared in the project to display the additional text.
- 2 data stores:
  - More than one to ensure that there are no issues around unique keys etc.

## Generating README

This README is autogenerated using the code in `generate-readme-module.js`.
Autogeneration is used to allow a summary of the test results to be included within the README.

The creation of the summary of results is handled by the code within `generate-readme-module.js` and is then injected into a markdown template at `resources/readme-template.md`.
Therefore, to make changes to the generated output, you need to make the changes in one of these places.

The README is generated in one of two ways:

1. As a part of running the `lighthouse-testing` script. This is run to inject the latest results into the readme.
1. By running the `generate-readme-standalone.js` script. This can be run when any of the surrounding information that doesn't require new test results is updated. The script can be executed using `node generate-readme-standalone.js`.

When writing the `readme-template.md`, an approach is being taken to have a single sentence per line of the file where possible (i.e. this doesn't work nicely for numbered/bulleted lists).
This should help minimise the git diff for any changes that are made to the template.

The final `README.md` file is formatted with `prettier` to help iron out any formatting issues that might arise when creating the autogenerated content and merging that with the wider README template.
