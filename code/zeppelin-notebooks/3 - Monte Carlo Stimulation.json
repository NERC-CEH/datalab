{"paragraphs":[{"title":"Introduction","text":"%md\nThis SparkR workbook loads the combined data set performs a Monte Carlo stimulation. The number of stimulations is defined by the `runCount` variable, this has successfully worked for up to 40 runs. The combined data is expected to be in multiple csv files, the path to these files is defined by `inputPath`. The output for each stimulation run is a single CSV giving the predicted values for that model fitting and will be written to the `outputPath` directory. A temporary directory is provide to speed up the GLM model fitting. All the paths must be within the `/data/` directory.","user":"anonymous","dateUpdated":"2017-07-28T15:28:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>This SparkR workbook loads the combined data set performs a Monte Carlo stimulation. The number of stimulations is defined by the <code>runCount</code> variable, this has successfully worked for up to 40 runs. The combined data is expected to be in multiple csv files, the path to these files is defined by <code>inputPath</code>. The output for each stimulation run is a single CSV giving the predicted values for that model fitting and will be written to the <code>outputPath</code> directory. A temporary directory is provide to speed up the GLM model fitting. All the paths must be within the <code>/data/</code> directory.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501253878364_-1977241747","id":"20170726-154559_882096536","dateCreated":"2017-07-28T14:57:58+0000","dateStarted":"2017-07-28T15:28:28+0000","dateFinished":"2017-07-28T15:28:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4647"},{"title":"Define Variables for Monte Carlo Stimulation","text":"%r\nrunCount = 10\ninputPath = \"/data/results/lcm_combined/part*\"\noutputPath = \"/data/results/spark_glm_monte_carlo\"\ntmpPath = \"/data/tmp/monte_carlo_tmp\"","dateUpdated":"2017-07-28T15:02:17+0000","config":{"editorSetting":{"language":"r"},"colWidth":12,"editorMode":"ace/mode/r","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"\n\n\n\n"}]},"apps":[],"jobName":"paragraph_1501253878366_-1976472249","id":"20170726-141714_1246743297","dateCreated":"2017-07-28T14:57:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4648"},{"title":"Load Combined Land Cover Map and Survey Dataset","text":"%r\n# magrittr library provides piping via %>%\nlibrary(magrittr)\n\n# Expected processed Land Cover Map data schema\ntotrich.wide.schema <- structType(structField(\"TOTRICH78\", \"integer\"), structField(\"TOTRICH90\", \"integer\"), structField(\"TOTRICH98\", \"integer\"), structField(\"TOTRICH07\", \"integer\"), \n\tstructField(\"PIX_DIST\", \"string\"), structField(\"MODAL_CLASS\", \"integer\"), structField(\"REPEAT_PLO\", \"string\"))\n\n# Read Land Cover Map data to Spark Data Frame - wide table\ntotrich.wide <- read.df(path = inputPath, source = \"csv\", schema = totrich.wide.schema)\n\n# Drop \"REPEAT_PLO\" duplicates\ntotrich.wide.trim <- totrich.wide %>% dropDuplicates(\"REPEAT_PLO\")","dateUpdated":"2017-07-28T14:57:58+0000","config":{"editorSetting":{"language":"r"},"colWidth":12,"editorMode":"ace/mode/r","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501253878367_-1976856998","id":"20170726-142002_959285706","dateCreated":"2017-07-28T14:57:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4649"},{"title":"Reshape Data","text":"%r\n# Function to restucture to long table\nreshapeTR <- function(columnYear) {\n\tcolnName <- columnYear[1]\n\tyrValue <- columnYear[2]\n    longTable <- totrich.wide.trim %>% select(colnName, \"PIX_DIST\", \"MODAL_CLASS\", \"REPEAT_PLO\") %>%\n    \twithColumn(\"YR\", lit(yrValue)) %>% withColumnRenamed(colnName, \"TOTRICH\")\n    longTable\n}\n\n# Restucture Land Cover Map data to long table\ncolumnYearLabels <- list(c(\"TOTRICH78\", \"1978\"), c(\"TOTRICH90\", \"1990\"), c(\"TOTRICH98\", \"1998\"), c(\"TOTRICH07\", \"2007\"))\ntotrich <- do.call(rbind, lapply(columnYearLabels, reshapeTR))\n","dateUpdated":"2017-07-28T14:57:58+0000","config":{"editorSetting":{"language":"r"},"colWidth":12,"editorMode":"ace/mode/r","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"\n\n\n\n"}]},"apps":[],"jobName":"paragraph_1501253878368_-1991092707","id":"20170726-142839_1357105245","dateCreated":"2017-07-28T14:57:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4650"},{"title":"Define the Pixel Distribution Sampler Function","text":"%r\n# Function to resample pixel distributions using SparkR tools\npixDistSampler <- function(longTable) {\n    # Input is a fragment of a Spark Data Frame\n    nrows <- nrow(longTable)\n    for(i in 1:nrows){\n        row <- longTable[i,]\n        pixDist.counts <- as.double(unlist(strsplit(row$PIX_DIST, \",\")))\n        pixDist.randCount <- sum(pixDist.counts) * row$RAND\n        pixDist.csum <- cumsum(pixDist.counts)\n        pixDist.idx <- which(pixDist.csum >= pixDist.randCount)\n        longTable[i,6] <-  as.integer(pixDist.idx[1])\n    }\n    longTable\n}\n\n# Schema for the output from the resampler\npixDistSampler.schema <- structType(structField(\"TOTRICH\", \"integer\"), structField(\"PIX_DIST\", \"string\"),  structField(\"YR\", \"string\"), structField(\"REPEAT_PLO\", \"string\"),\n\tstructField(\"RAND\", \"double\"), structField(\"MODAL_CLASS\", \"integer\"))","dateUpdated":"2017-07-28T14:57:58+0000","config":{"editorSetting":{"language":"r"},"colWidth":12,"editorMode":"ace/mode/r","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"\n\n\n\n"}]},"apps":[],"jobName":"paragraph_1501253878369_-1991477456","id":"20170726-142948_533491255","dateCreated":"2017-07-28T14:57:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4651"},{"title":"Spark GLM","text":"%md\nWhen using R in Zeppelin the library `SparkR` is loaded in the background, this masks some underlying base R functions, most notable the `glm` function is now `SparkR::glm`. This function utilises the `MLlib` Spark library which is very fast for large datasets, but lacks functionality seen with base R glm function. Currently only a few formula operators are supported, including `~`, `.`, `:`, `+`, and `-`. There the interaction `A*B` shorthand is not recognised and has to be expanded to be `A+B+A:B`. Furthermore, R style factors can not be applied to Spark Data Frames, however, the same effect can be achieve by converting the column to strings. Finally, only `binomial`, `gaussian`, `Gamma`, and `poisson` families will work with `SparkR::glm`. ","dateUpdated":"2017-07-28T14:57:58+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>When using R in Zeppelin the library <code>SparkR</code> is loaded in the background, this masks some underlying base R functions, most notable the <code>glm</code> function is now <code>SparkR::glm</code>. This function utilises the <code>MLlib</code> Spark library which is very fast for large datasets, but lacks functionality seen with base R glm function. Currently only a few formula operators are supported, including <code>~</code>, <code>.</code>, <code>:</code>, <code>+</code>, and <code>-</code>. There the interaction <code>A*B</code> shorthand is not recognised and has to be expanded to be <code>A+B+A:B</code>. Furthermore, R style factors can not be applied to Spark Data Frames, however, the same effect can be achieve by converting the column to strings. Finally, only <code>binomial</code>, <code>gaussian</code>, <code>Gamma</code>, and <code>poisson</code> families will work with <code>SparkR::glm</code>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501253878370_-1990323209","id":"20170726-141639_1120369945","dateCreated":"2017-07-28T14:57:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4652"},{"title":"Define Monte Carlo Stimulation Function","text":"%r\nsparkGlmMC <- function(runIdx) {\n    runSelect <- totrich %>% select(\"TOTRICH\", \"PIX_DIST\", \"YR\", \"REPEAT_PLO\") %>% withColumn(\"RAND\", rand())\n    runData <- runSelect %>% dapply(pixDistSampler, pixDistSampler.schema)\n    runDataMut <- runData %>% mutate(MODAL_CLASS = cast(runData$MODAL_CLASS, \"string\"), YEAR = cast(runData$YR, \"integer\"))\n    runPredData <- distinct(select(runDataMut, \"MODAL_CLASS\", \"YR\"))\n    # Writing files before call GLM dramatically decreases loop duration\n    write.df(runDataMut, path = tmpPath, source = \"csv\", mode = \"overwrite\")\n    # Reload data from disk\n    runDataCollected <- read.df(path = tmpPath, source = \"csv\", schema = schema(runDataMut))\n    runFit <- glm(TOTRICH~MODAL_CLASS+YR+MODAL_CLASS:YR, family = poisson, data = runDataCollected)\n    runOutput <- predict(runFit, newData = runPredData)\n    # Append runOutput\n    repart <- repartition(runOutput, 1)\n    write.df(repart, path=outputPath, source=\"csv\", mode=\"append\")\n}","dateUpdated":"2017-07-28T14:57:58+0000","config":{"editorSetting":{"language":"r"},"colWidth":12,"editorMode":"ace/mode/r","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"\n\n\n\n"}]},"apps":[],"jobName":"paragraph_1501253878370_-1990323209","id":"20170726-143238_1158526379","dateCreated":"2017-07-28T14:57:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4653"},{"title":"Run the Monte Carlo Stimulation","text":"%r\n# SparkR lapply will parallelise the Monte Carlo runs\npqlMCOutput <- lapply(seq(runCount), sparkGlmMC)","dateUpdated":"2017-07-28T14:57:58+0000","config":{"editorSetting":{"language":"r"},"colWidth":12,"editorMode":"ace/mode/r","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501253878371_-1990707958","id":"20170726-143720_62470571","dateCreated":"2017-07-28T14:57:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4654"},{"text":"%r\n","dateUpdated":"2017-07-28T14:57:58+0000","config":{"colWidth":12,"editorMode":"ace/mode/r","results":{},"enabled":true,"editorSetting":{"language":"r"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501253878372_-1992631703","id":"20170726-161301_2084911664","dateCreated":"2017-07-28T14:57:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4655"}],"name":"3 - Monte Carlo Stimulation","id":"2CQNS29QF","angularObjects":{"2CRJXYZ4A:shared_process":[],"2CP3GRG8J:shared_process":[],"2CPERBXDR:shared_process":[],"2CQT4R4HK:shared_process":[],"2CS4TE2YA:shared_process":[],"2CR34MNNR:shared_process":[],"2CPF7YH2C:shared_process":[],"2CN8GHQ4F:shared_process":[],"2CNUGKAPV:shared_process":[],"2CPZ5Q3A3:shared_process":[],"2CRMWD5ZD:shared_process":[],"2CS2QET1R:shared_process":[],"2CN7DPMYX:shared_process":[],"2CQGHBWCK:shared_process":[],"2CPJG2UAF:shared_process":[],"2CRCTT4SH:shared_process":[],"2CQ42UGXN:shared_process":[],"2CPVVYTEH:shared_process":[],"2CRKJD72C:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}