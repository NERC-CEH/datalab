{"paragraphs":[{"title":"Introduction","text":"%md\nThis scala workbook loads and aggregates the output from the Monte Carlo stimulations, by calculating the mean and variance. The aggregated values are merged with the Land Cover Map (LCM). The Monte Carl data is expected to be in multiple csv files (`resultsFilePath`) and the LCM (`lcmFilePath`) is expected to be in multiple well-known text (wkt) formatted text files. The output from this workbook will be multiple well-known text (wkt) formatted text files which include the mean and variance values. All the paths must be within the /data/ directory.","dateUpdated":"2017-08-01T11:41:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>This scala workbook loads and aggregates the output from the Monte Carlo stimulations, by calculating the mean and variance. The aggregated values are merged with the Land Cover Map (LCM). The Monte Carl data is expected to be in multiple csv files (<code>resultsFilePath</code>) and the LCM (<code>lcmFilePath</code>) is expected to be in multiple well-known text (wkt) formatted text files. The output from this workbook will be multiple well-known text (wkt) formatted text files which include the mean and variance values. All the paths must be within the /data/ directory.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1501587660502_-1047530629","id":"20170728-152730_1893452821","dateCreated":"2017-08-01T11:41:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:262"},{"title":"Define paths for GLM Output and Land Cover Map","text":"val resultsFilePath = \"/data/lcm/results/spark_glm_monte_carlo/part*\"\nval lcmFilePath = \"/data/lcm/maps/csv/lcm25mWKT.*\"\nval outputPath = \"/data/lcm/results/aggregated_richness\"","dateUpdated":"2017-08-04T15:47:27+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nresultsFilePath: String = /data/lcm/results/spark_glm_monte_carlo/part*\n\nlcmFilePath: String = /data/lcm/maps/csv/lcm25mWKT.*\n\noutputPath: String = /data/lcm/results/aggregated_richness\n"}]},"apps":[],"jobName":"paragraph_1501587660503_-1047915378","id":"20170728-150738_256870943","dateCreated":"2017-08-01T11:41:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:263","user":"datalab","dateFinished":"2017-08-04T15:47:27+0000","dateStarted":"2017-08-04T15:47:27+0000"},{"title":"Create Case Classes for Data Import","text":"import org.apache.spark.sql.{Row, SparkSession}\nimport org.apache.spark.sql.types.{DoubleType, IntegerType, StructField, StructType}\nimport org.apache.spark.sql.functions._\n\ncase class ResultRow(lcmClass: Int, year: Int, predictedRichness: Double)\ncase class OutputFeature(geometry: String, modalClass: Int, richness: Array[(Double, Double)])","dateUpdated":"2017-08-01T11:41:00+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.sql.{Row, SparkSession}\n\nimport org.apache.spark.sql.types.{DoubleType, IntegerType, StructField, StructType}\n\nimport org.apache.spark.sql.functions._\n\ndefined class ResultRow\n\ndefined class OutputFeature\n"}]},"apps":[],"jobName":"paragraph_1501587660504_-1049839122","id":"20170620-143527_1759559639","dateCreated":"2017-08-01T11:41:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:264"},{"title":"Create functions for data processing","text":"def parseFeature(row: Row) : OutputFeature = {\n    val geometryString = row.getAs[String](\"_c0\")\n    val modalClass = row.getAs[String](\"_c7\").toInt\n    \n    OutputFeature(geometryString, modalClass, null)\n}\n\ndef getRowForYearAndModalClass(year: Int, modalClass: Int,\n                             lookup: Array[((Int, Int), Double, Double)]): Option[((Int, Int), Double, Double)] = {\n    lookup.find(row => row._1._1 == year && row._1._2 == modalClass)\n}\n\ndef getPredictedValueForYearAndModalClass(year: Int, modalClass: Int,\n                                        lookup: Array[((Int, Int), Double, Double)]) : Double = {\n    val row = getRowForYearAndModalClass(year, modalClass, lookup)\n    if (row.isDefined) {\n      row.get._2\n    } else {\n      0\n    }\n}\n\ndef getVarianceForYearAndModalClass(year: Int, modalClass: Int,\n                                        lookup: Array[((Int, Int), Double, Double)]) : Double = {\n    val row = getRowForYearAndModalClass(year, modalClass, lookup)\n    if (row.isDefined) {\n      row.get._3\n    } else {\n      0\n    }\n}","dateUpdated":"2017-08-01T11:41:00+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nparseFeature: (row: org.apache.spark.sql.Row)OutputFeature\n\ngetRowForYearAndModalClass: (year: Int, modalClass: Int, lookup: Array[((Int, Int), Double, Double)])Option[((Int, Int), Double, Double)]\n\ngetPredictedValueForYearAndModalClass: (year: Int, modalClass: Int, lookup: Array[((Int, Int), Double, Double)])Double\n\ngetVarianceForYearAndModalClass: (year: Int, modalClass: Int, lookup: Array[((Int, Int), Double, Double)])Double\n"}]},"apps":[],"jobName":"paragraph_1501587660504_-1049839122","id":"20170620-143555_28800291","dateCreated":"2017-08-01T11:41:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:265"},{"title":"Create Aggregated Lookup Table","text":"import spark.implicits._\n\nval schema = StructType(Array(\n  StructField(\"lcmClass\", IntegerType),\n  StructField(\"year\", IntegerType),\n  StructField(\"predictedRichness\", DoubleType)))\n\nval resultDataset = spark.sqlContext.read.format(\"csv\").schema(schema).csv(resultsFilePath).as[ResultRow]\nval resultLookup = resultDataset\n  .groupByKey(x => (x.year, x.lcmClass))\n  .agg(avg(\"predictedRichness\").as[Double], variance(\"predictedRichness\").as[Double])\n  .cache()\n\nval lookup = resultLookup.collect()","dateUpdated":"2017-08-01T11:41:00+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport spark.implicits._\n\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(lcmClass,IntegerType,true), StructField(year,IntegerType,true), StructField(predictedRichness,DoubleType,true))\n\nresultDataset: org.apache.spark.sql.Dataset[ResultRow] = [lcmClass: int, year: int ... 1 more field]\n\nresultLookup: org.apache.spark.sql.Dataset[((Int, Int), Double, Double)] = [key: struct<_1: int, _2: int>, avg(predictedRichness): double ... 1 more field]\nlookup: Array[((Int, Int), Double, Double)] = Array(((1990,7),7.835412538652726,0.05400676415613198), ((1998,2),12.279904873956982,0.07925253753609061), ((2007,6),14.677986287093903,3.064932749301545), ((1998,7),12.37853696392409,0.051787858875581544), ((1990,4),8.847586986869258,0.01640895462482512), ((1990,6),11.223498823019398,3.1770933768702436), ((1978,7),2.813002908866241,0.04127107868167925), ((1998,1),11.164871318048803,0.1131152627252787), ((1978,5),1.8520776025918848,1.4323629923275094), ((1998,13),11.50060554004179,0.7153018653639063), ((1990,15),12.042967763484649,4.757037821122775), ((1978,21),3.1866207471283023,0.10511850659512541), ((1978,15),4.954644757876869,4.447922437524927), ((2007,1),12.96112423491948,0.05734182713708559), ((2007,10),13.482477964480754,0.05273281322..."}]},"apps":[],"jobName":"paragraph_1501587660505_-1050223871","id":"20170620-143713_707572954","dateCreated":"2017-08-01T11:41:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:266"},{"title":"Augment Land Cover Map with Predicted values","text":"// Broadcast lookup table to nodes\nval bLookup = spark.sparkContext.broadcast(lookup)\n\nval lcmReader = spark.read.csv(lcmFilePath)\nval lcmData = lcmReader.rdd.map(row => parseFeature(row))\n\nval predictedMap = lcmData.map(feature => {\n  val years = Array(1978, 1990, 1998, 2007)\n\n  val richness = years.map(year => {\n    val predictedRichness = getPredictedValueForYearAndModalClass(year, feature.modalClass, bLookup.value)\n    val variance = getVarianceForYearAndModalClass(year, feature.modalClass, bLookup.value)\n\n    (predictedRichness, variance)\n  })\n\n  OutputFeature(feature.geometry, feature.modalClass, richness)\n})","dateUpdated":"2017-08-01T11:41:00+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nbLookup: org.apache.spark.broadcast.Broadcast[Array[((Int, Int), Double, Double)]] = Broadcast(5)\n\nlcmReader: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 10 more fields]\n\nlcmData: org.apache.spark.rdd.RDD[OutputFeature] = MapPartitionsRDD[26] at map at <console>:41\n\npredictedMap: org.apache.spark.rdd.RDD[OutputFeature] = MapPartitionsRDD[27] at map at <console>:64\n"}]},"apps":[],"jobName":"paragraph_1501587660505_-1050223871","id":"20170620-143808_860005175","dateCreated":"2017-08-01T11:41:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:267"},{"title":"Write data to file","text":"predictedMap.map(row => {\n  val geometryString = \"\\\"\" + row.geometry + \"\\\"\"\n  val richnessValues = row.richness.flatMap(r => List(r._1, r._2)).mkString(\",\")\n  val outputVals = Array(geometryString, row.modalClass, richnessValues)\n  outputVals.mkString(\",\")\n}).saveAsTextFile(outputPath)","dateUpdated":"2017-08-01T11:41:00+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1501587660506_-1049069624","id":"20170620-143850_210570896","dateCreated":"2017-08-01T11:41:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:268"},{"dateUpdated":"2017-08-01T11:41:00+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1501587660506_-1049069624","id":"20170620-144139_1516321013","dateCreated":"2017-08-01T11:41:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:269"}],"name":"LCM/4 - Create Output Map Data","id":"2CRANSJB6","angularObjects":{"2CNGRH8TD:shared_process":[],"2CPW6GTBS:shared_process":[],"2CNVUFKB1:shared_process":[],"2CSB82229:shared_process":[],"2CQX9WRMS:shared_process":[],"2CQEXWV1K:shared_process":[],"2CNNAQR9H:shared_process":[],"2CSE4E46W:shared_process":[],"2CQX7T5GY:shared_process":[],"2CS41AD7V:shared_process":[],"2CQEKSAGW:shared_process":[],"2CNS9N7NH:shared_process":[],"2CP7GVTNW:shared_process":[],"2CQ32EKTA:shared_process":[],"2CRQD8UW8:shared_process":[],"2CR8SP4NR:shared_process":[],"2CNHW727V:shared_process":[],"2CQQ5Q35M:shared_process":[],"2CPTY6BP1:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}